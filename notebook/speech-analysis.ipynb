{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract info from audio files\n",
    "def extract_audio_features(file_name, mfcc, chroma, mel):\n",
    "    \"\"\"\n",
    "    mfcc represents the short term power spectrum of the sound\n",
    "    chroma is the pitch\n",
    "    mel is the spectrogram frequency\n",
    "    \"\"\"\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            fourier = np.abs(librosa.stft(X))\n",
    "            \n",
    "        # compile the three features into a result    \n",
    "        result = np.array([])\n",
    "\n",
    "        if mfcc:\n",
    "            pwr_spec = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, pwr_spec)) # add to result\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=fourier, \n",
    "                                                        sr=sample_rate,\n",
    "                                                        ).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the emotion labels in the RAVDESS dataset\n",
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',    \n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',    \n",
    "    '08': 'surprised' \n",
    "    }\n",
    "# we are looking at a subset of emotions of interest    \n",
    "observed_emotions = ('sad', 'happy', 'fearful', 'surprised' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the files in. \n",
    "# We needed the previous functions to make sense of the filename and info\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    x,y = [], []\n",
    "    # use the glob library to parse through files with wildcard\n",
    "    files = [file for file in glob.glob(\"..\\data\\Actor_*\\*.wav\")]\n",
    "    for file in files:\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature = extract_audio_features(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is a rather conservative split given the amount of data.\n",
    "# First I will run it as is but then I want to train on more of the data \n",
    "# Could use data augmentation to beef it up.\n",
    "X_train, X_test, y_train, y_test = load_data(test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'> <class 'list'>\nshapes of data: ((614, 180), (154, 180), (154,)) (614,)\nThe first number represents how many files, the second is how many feautures extracted\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), type(y_test))\n",
    "print('shapes of data:', (np.array(X_train).shape, np.array(X_test).shape, np.array(y_test).shape), np.shape(y_train))\n",
    "print('The first number represents how many files, the second is how many feautures extracted' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method shows here is from sklearn's neural network... could potentially try our own later\n",
    "# Multi Layer Perceptron Classifier\n",
    "model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(500,), learning_rate='adaptive', max_iter=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 20.89579859\n",
      "Iteration 2, loss = 9.48402521\n",
      "Iteration 3, loss = 9.23242576\n",
      "Iteration 4, loss = 7.20703311\n",
      "Iteration 5, loss = 3.76408598\n",
      "Iteration 6, loss = 4.07899478\n",
      "Iteration 7, loss = 3.28271669\n",
      "Iteration 8, loss = 3.79703887\n",
      "Iteration 9, loss = 2.02600256\n",
      "Iteration 10, loss = 2.34710678\n",
      "Iteration 11, loss = 1.77742139\n",
      "Iteration 12, loss = 1.43767627\n",
      "Iteration 13, loss = 1.59180284\n",
      "Iteration 14, loss = 1.33044353\n",
      "Iteration 15, loss = 1.23131094\n",
      "Iteration 16, loss = 1.17443287\n",
      "Iteration 17, loss = 1.11062840\n",
      "Iteration 18, loss = 1.05289812\n",
      "Iteration 19, loss = 1.03571937\n",
      "Iteration 20, loss = 1.04958473\n",
      "Iteration 21, loss = 0.98199835\n",
      "Iteration 22, loss = 1.06285670\n",
      "Iteration 23, loss = 1.11102844\n",
      "Iteration 24, loss = 0.97210655\n",
      "Iteration 25, loss = 1.03939402\n",
      "Iteration 26, loss = 0.94622273\n",
      "Iteration 27, loss = 0.93425317\n",
      "Iteration 28, loss = 0.82279339\n",
      "Iteration 29, loss = 0.79999273\n",
      "Iteration 30, loss = 0.75767514\n",
      "Iteration 31, loss = 0.74844225\n",
      "Iteration 32, loss = 0.74040972\n",
      "Iteration 33, loss = 0.73246367\n",
      "Iteration 34, loss = 0.70857492\n",
      "Iteration 35, loss = 0.75459945\n",
      "Iteration 36, loss = 0.74612565\n",
      "Iteration 37, loss = 0.71481041\n",
      "Iteration 38, loss = 0.72027329\n",
      "Iteration 39, loss = 0.68733185\n",
      "Iteration 40, loss = 0.71488343\n",
      "Iteration 41, loss = 0.71501225\n",
      "Iteration 42, loss = 0.69327466\n",
      "Iteration 43, loss = 0.68797373\n",
      "Iteration 44, loss = 0.64286864\n",
      "Iteration 45, loss = 0.64442990\n",
      "Iteration 46, loss = 0.60651786\n",
      "Iteration 47, loss = 0.58424558\n",
      "Iteration 48, loss = 0.58368891\n",
      "Iteration 49, loss = 0.62319131\n",
      "Iteration 50, loss = 0.62373679\n",
      "Iteration 51, loss = 0.62765169\n",
      "Iteration 52, loss = 0.56773698\n",
      "Iteration 53, loss = 0.56574075\n",
      "Iteration 54, loss = 0.59024369\n",
      "Iteration 55, loss = 0.58367505\n",
      "Iteration 56, loss = 0.57465069\n",
      "Iteration 57, loss = 0.55072706\n",
      "Iteration 58, loss = 0.57801178\n",
      "Iteration 59, loss = 0.56382303\n",
      "Iteration 60, loss = 0.52522923\n",
      "Iteration 61, loss = 0.53119633\n",
      "Iteration 62, loss = 0.53284490\n",
      "Iteration 63, loss = 0.49734223\n",
      "Iteration 64, loss = 0.49096590\n",
      "Iteration 65, loss = 0.49805164\n",
      "Iteration 66, loss = 0.49052495\n",
      "Iteration 67, loss = 0.47056732\n",
      "Iteration 68, loss = 0.46689236\n",
      "Iteration 69, loss = 0.46012098\n",
      "Iteration 70, loss = 0.46306998\n",
      "Iteration 71, loss = 0.46051826\n",
      "Iteration 72, loss = 0.46106465\n",
      "Iteration 73, loss = 0.46063235\n",
      "Iteration 74, loss = 0.45705697\n",
      "Iteration 75, loss = 0.42995744\n",
      "Iteration 76, loss = 0.45416521\n",
      "Iteration 77, loss = 0.47491075\n",
      "Iteration 78, loss = 0.41046520\n",
      "Iteration 79, loss = 0.44795006\n",
      "Iteration 80, loss = 0.40545847\n",
      "Iteration 81, loss = 0.41755820\n",
      "Iteration 82, loss = 0.41105979\n",
      "Iteration 83, loss = 0.44757355\n",
      "Iteration 84, loss = 0.50490321\n",
      "Iteration 85, loss = 0.51212982\n",
      "Iteration 86, loss = 0.48779287\n",
      "Iteration 87, loss = 0.46276892\n",
      "Iteration 88, loss = 0.43658899\n",
      "Iteration 89, loss = 0.38546133\n",
      "Iteration 90, loss = 0.40023390\n",
      "Iteration 91, loss = 0.39382121\n",
      "Iteration 92, loss = 0.37008698\n",
      "Iteration 93, loss = 0.36844417\n",
      "Iteration 94, loss = 0.38356173\n",
      "Iteration 95, loss = 0.38717902\n",
      "Iteration 96, loss = 0.41207120\n",
      "Iteration 97, loss = 0.41118408\n",
      "Iteration 98, loss = 0.38517762\n",
      "Iteration 99, loss = 0.38771828\n",
      "Iteration 100, loss = 0.35105945\n",
      "Iteration 101, loss = 0.35059132\n",
      "Iteration 102, loss = 0.35385815\n",
      "Iteration 103, loss = 0.39009900\n",
      "Iteration 104, loss = 0.35968334\n",
      "Iteration 105, loss = 0.35737069\n",
      "Iteration 106, loss = 0.35918152\n",
      "Iteration 107, loss = 0.39360989\n",
      "Iteration 108, loss = 0.35892888\n",
      "Iteration 109, loss = 0.35149135\n",
      "Iteration 110, loss = 0.32863541\n",
      "Iteration 111, loss = 0.30800695\n",
      "Iteration 112, loss = 0.34725734\n",
      "Iteration 113, loss = 0.33956168\n",
      "Iteration 114, loss = 0.33449792\n",
      "Iteration 115, loss = 0.33455089\n",
      "Iteration 116, loss = 0.29929741\n",
      "Iteration 117, loss = 0.29888881\n",
      "Iteration 118, loss = 0.32873687\n",
      "Iteration 119, loss = 0.31096835\n",
      "Iteration 120, loss = 0.28693615\n",
      "Iteration 121, loss = 0.28538331\n",
      "Iteration 122, loss = 0.31352367\n",
      "Iteration 123, loss = 0.28878743\n",
      "Iteration 124, loss = 0.29192903\n",
      "Iteration 125, loss = 0.29786235\n",
      "Iteration 126, loss = 0.30733089\n",
      "Iteration 127, loss = 0.28214074\n",
      "Iteration 128, loss = 0.28305557\n",
      "Iteration 129, loss = 0.28722310\n",
      "Iteration 130, loss = 0.26077389\n",
      "Iteration 131, loss = 0.25120921\n",
      "Iteration 132, loss = 0.27239744\n",
      "Iteration 133, loss = 0.26120773\n",
      "Iteration 134, loss = 0.26179147\n",
      "Iteration 135, loss = 0.27563690\n",
      "Iteration 136, loss = 0.28119797\n",
      "Iteration 137, loss = 0.26805919\n",
      "Iteration 138, loss = 0.28569458\n",
      "Iteration 139, loss = 0.29798151\n",
      "Iteration 140, loss = 0.30972857\n",
      "Iteration 141, loss = 0.27562630\n",
      "Iteration 142, loss = 0.24372942\n",
      "Iteration 143, loss = 0.24324873\n",
      "Iteration 144, loss = 0.22764461\n",
      "Iteration 145, loss = 0.22807422\n",
      "Iteration 146, loss = 0.22123760\n",
      "Iteration 147, loss = 0.21654968\n",
      "Iteration 148, loss = 0.21861776\n",
      "Iteration 149, loss = 0.21543863\n",
      "Iteration 150, loss = 0.24390409\n",
      "Iteration 151, loss = 0.21266968\n",
      "Iteration 152, loss = 0.20410153\n",
      "Iteration 153, loss = 0.21019644\n",
      "Iteration 154, loss = 0.23012679\n",
      "Iteration 155, loss = 0.22347151\n",
      "Iteration 156, loss = 0.22576078\n",
      "Iteration 157, loss = 0.23922380\n",
      "Iteration 158, loss = 0.24162977\n",
      "Iteration 159, loss = 0.22014957\n",
      "Iteration 160, loss = 0.20361124\n",
      "Iteration 161, loss = 0.18809071\n",
      "Iteration 162, loss = 0.18911946\n",
      "Iteration 163, loss = 0.20065583\n",
      "Iteration 164, loss = 0.18991281\n",
      "Iteration 165, loss = 0.18354366\n",
      "Iteration 166, loss = 0.17554614\n",
      "Iteration 167, loss = 0.18054574\n",
      "Iteration 168, loss = 0.17448107\n",
      "Iteration 169, loss = 0.19107949\n",
      "Iteration 170, loss = 0.18960652\n",
      "Iteration 171, loss = 0.20090592\n",
      "Iteration 172, loss = 0.18361486\n",
      "Iteration 173, loss = 0.19810542\n",
      "Iteration 174, loss = 0.18232950\n",
      "Iteration 175, loss = 0.18947923\n",
      "Iteration 176, loss = 0.18700972\n",
      "Iteration 177, loss = 0.19135612\n",
      "Iteration 178, loss = 0.19452273\n",
      "Iteration 179, loss = 0.21102403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(500,),\n",
       "              learning_rate='adaptive', max_iter=500, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.708\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:', round(accuracy_score(y_true=y_test, y_pred=y_pred), ndigits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try reading in our own sample\n",
    "audio_sample = extract_audio_features('C:\\\\Users\\\\jonma\\\\Programming\\\\speech-emotion\\\\fearful-sample.wav', mfcc=True, chroma=True, mel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['fearful'], dtype='<U7')"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "model.predict(np.array(audio_sample).reshape(1, -1))\n",
    "# Hooray!!! It worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['fearful'], dtype='<U7')"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "audio_sample1 = extract_audio_features('C:\\\\Users\\\\jonma\\\\Programming\\\\speech-emotion\\\\disgust-sample.wav', mfcc=True, chroma=True, mel=True)\n",
    "model.predict(np.array(audio_sample1).reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-3.22932892e+02,  1.03630600e+02,  2.96217728e+01,  3.68155708e+01,\n",
       "       -6.88917446e+00,  4.83659697e+00,  4.83311462e+00,  3.57040167e+00,\n",
       "        1.76188755e+01, -1.09492791e+00,  5.21700001e+00,  5.98082244e-01,\n",
       "        1.02813768e+00, -1.44653046e+00,  2.94739389e+00,  6.23048735e+00,\n",
       "        6.38736343e+00,  4.91027069e+00, -1.40401721e+00,  1.64358902e+00,\n",
       "       -2.96373874e-01, -5.16425073e-01,  1.78830469e+00,  7.10774004e-01,\n",
       "       -2.85262465e+00, -3.83084249e+00, -1.48065722e+00,  3.52736473e-01,\n",
       "       -6.22448683e-01, -2.50782728e+00, -2.39144039e+00,  2.01076221e+00,\n",
       "        3.39728475e+00,  2.69857788e+00,  2.67807436e+00,  1.47958291e+00,\n",
       "        1.31601512e+00,  1.15661609e+00, -9.31499898e-02,  1.58808470e+00,\n",
       "        7.86037624e-01,  7.43378818e-01,  6.74417853e-01,  6.02336347e-01,\n",
       "        5.94220161e-01,  5.97750902e-01,  6.23982131e-01,  6.72653854e-01,\n",
       "        7.12833345e-01,  7.61273503e-01,  7.61011541e-01,  7.86074996e-01,\n",
       "        7.22769260e+00,  1.88746035e-01,  9.48401928e-01,  1.19802022e+00,\n",
       "        9.54798639e-01,  2.84737420e+00,  5.08687115e+00,  1.82175863e+00,\n",
       "        9.33051050e-01,  1.00234783e+00,  1.41669428e+00,  7.15833127e-01,\n",
       "        8.75241995e-01,  3.74418950e+00,  9.01836300e+00,  4.54879808e+00,\n",
       "        2.20187545e+00,  2.64189959e+00,  1.64057875e+00,  5.63292623e-01,\n",
       "        7.13719308e-01,  1.24503100e+00,  1.07888281e+00,  4.96687651e-01,\n",
       "        6.09215021e-01,  6.98989689e-01,  5.69760501e-01,  6.00020409e-01,\n",
       "        7.98986256e-01,  8.44548702e-01,  9.47558105e-01,  1.67881835e+00,\n",
       "        9.72545922e-01,  7.55028665e-01,  1.25522268e+00,  1.06863081e+00,\n",
       "        2.36994267e-01,  7.60031864e-02,  5.54163940e-02,  6.43245205e-02,\n",
       "        3.59035768e-02,  7.56992679e-03,  4.78113862e-03,  6.11027377e-03,\n",
       "        2.35310420e-02,  2.90462729e-02,  2.47560889e-02,  2.76045017e-02,\n",
       "        7.83743188e-02,  3.56410593e-02,  7.66365463e-03,  2.60034762e-03,\n",
       "        3.48706078e-03,  4.66647698e-03,  1.12103261e-02,  1.26793161e-02,\n",
       "        1.24860434e-02,  8.13377555e-03,  7.19390903e-03,  6.45171246e-03,\n",
       "        9.05333646e-03,  5.46271540e-03,  7.60201924e-03,  4.99036675e-03,\n",
       "        2.00117454e-02,  3.22535411e-02,  1.47597631e-02,  1.42926304e-02,\n",
       "        1.72570199e-02,  6.85366103e-03,  6.66255923e-03,  6.64932607e-03,\n",
       "        6.47187885e-03,  5.23015065e-03,  4.72467160e-03,  6.27797982e-03,\n",
       "        1.10252742e-02,  1.20398942e-02,  2.61726733e-02,  3.26803662e-02,\n",
       "        2.46046633e-02,  1.86743252e-02,  3.40181105e-02,  3.02980896e-02,\n",
       "        3.23040634e-02,  3.77912037e-02,  2.17907056e-02,  3.24696116e-02,\n",
       "        5.31789660e-02,  7.00482577e-02,  3.19014080e-02,  3.10213324e-02,\n",
       "        1.02982491e-01,  6.52761459e-02,  4.84835804e-02,  4.37405929e-02,\n",
       "        3.29417847e-02,  4.16707769e-02,  3.85009870e-02,  1.60374176e-02,\n",
       "        1.43129155e-02,  1.26084993e-02,  7.84761738e-03,  6.79757306e-03,\n",
       "        5.79507183e-03,  3.79732461e-03,  2.40097288e-03,  1.80523400e-03,\n",
       "        1.43658218e-03,  9.24380962e-04,  5.87865361e-04,  2.99094769e-04,\n",
       "        2.48615805e-04,  1.90300707e-04,  1.50651787e-04,  1.07005428e-04,\n",
       "        8.51564764e-05,  9.77277086e-05,  1.07007996e-04,  1.15247596e-04,\n",
       "        1.12407652e-04,  1.11540838e-04,  1.17859825e-04,  1.15193921e-04,\n",
       "        1.14842609e-04,  1.05024206e-04,  9.56502336e-05,  7.76745801e-05])"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "audio_sample2 = extract_audio_features('C:\\\\Users\\\\jonma\\\\Programming\\\\speech-emotion\\\\extra-disgust-sample.wav', mfcc=True, chroma=True, mel=True)\n",
    "model.predict(np.array(audio_sample2).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['calm'], dtype='<U7')"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "audio_sample3 = extract_audio_features('C:\\\\Users\\\\jonma\\\\Programming\\\\speech-emotion\\\\calm-sample.wav', mfcc=True, chroma=True, mel=True)\n",
    "model.predict(np.array(audio_sample3).reshape(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-emotion",
   "language": "python",
   "name": "speech-emotion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}